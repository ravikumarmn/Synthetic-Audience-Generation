#!/usr/bin/env python3
"""
Synthetic Audience Generator MVP
Single-file implementation using LangChain + LangGraph + Gemini Flash 2.5

This MVP generates synthetic audience profiles with exact demographic distribution
matching while using LLM only for behavioral content generation.
"""

import json
import os
import re
import warnings
import time
import random
from typing import Dict, List, TypedDict, Optional, Tuple, Any
from dataclasses import dataclass
from collections import Counter
import logging

# Core dependencies
from pydantic import BaseModel, Field, field_validator
from dotenv import load_dotenv
from tqdm import tqdm
import click

# LangChain and LangGraph
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, END

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ============================================================================
# CORE DATA MODELS
# ============================================================================


class FilterDetails(BaseModel):
    """Input filter details from JSON request."""

    user_req_responses: int = Field(
        ..., gt=0, description="Number of responses required"
    )
    age_filter: Dict[str, int] = Field(..., description="Age bounds")
    age_proportions: Dict[str, int] = Field(..., description="Age group proportions")
    gender_filter: List[str] = Field(..., description="Allowed genders")
    gender_proportions: Dict[str, int] = Field(..., description="Gender proportions")
    ethnicity_filter: List[str] = Field(..., description="Allowed ethnicities")
    ethnicity_proportions: Dict[str, int] = Field(
        ..., description="Ethnicity proportions"
    )
    age_filter_complete: bool = Field(default=True)
    ethnicity_filter_complete: bool = Field(default=True)

    @field_validator("age_proportions", "gender_proportions", "ethnicity_proportions")
    @classmethod
    def validate_proportions_sum_to_100(cls, v):
        """Ensure proportions sum to 100."""
        total = sum(v.values())
        if total != 100:
            raise ValueError(f"Proportions must sum to 100, got {total}")
        return v


class InputPersona(BaseModel):
    """Input persona from JSON request."""

    id: int
    generatedTitle: str
    personaName: str
    age: int
    location: str
    ethnicity: str
    gender: str
    personaType: str
    about: str
    goalsAndMotivations: List[str]
    frustrations: List[str]
    needState: str
    occasions: str
    image: Optional[str] = None
    userId: Optional[int] = None
    createdAt: Optional[str] = None
    requestParams: Optional[Dict] = None


class RequestData(BaseModel):
    """Complete request data structure."""

    request: List[Dict[str, Any]]

    def get_filter_details(self) -> FilterDetails:
        """Extract filter details from request."""
        return FilterDetails(**self.request[0]["filter_details"])

    def get_personas(self) -> List[InputPersona]:
        """Extract personas from request."""
        return [InputPersona(**persona) for persona in self.request[0]["personas"]]


class DemographicAssignment(BaseModel):
    """Demographic assignment for a single profile."""

    age_bucket: str
    gender: str
    ethnicity: str
    profile_index: int


class BehavioralContent(BaseModel):
    """Generated behavioral content from LLM."""

    about: str = Field(..., description="About section")
    goalsAndMotivations: List[str] = Field(..., description="Goals and motivations")
    frustrations: List[str] = Field(..., description="Frustrations")
    needState: str = Field(..., description="Need state")
    occasions: str = Field(..., description="Occasions")


class SyntheticProfile(BaseModel):
    """Complete synthetic audience profile."""

    # Demographics (assigned by Python)
    age_bucket: str
    gender: str
    ethnicity: str

    # Behavioral content (generated by LLM)
    about: str
    goalsAndMotivations: List[str]
    frustrations: List[str]
    needState: str
    occasions: str

    # Metadata
    profile_id: int


class ProcessingTemplates(BaseModel):
    """Cleaned templates for LLM generation."""

    about_templates: List[str]
    goals_templates: List[str]
    frustrations_templates: List[str]
    need_state_templates: List[str]
    occasions_templates: List[str]


# ============================================================================
# LANGGRAPH STATE MANAGEMENT
# ============================================================================


class SyntheticAudienceState(TypedDict, total=False):
    """State management for LangGraph workflow."""

    # Input data (required)
    input_file: str
    output_file: str

    # Parsed data (added during workflow)
    filter_details: FilterDetails
    input_personas: List[InputPersona]

    # Processing data (added during workflow)
    demographic_schedule: List[DemographicAssignment]
    processing_templates: ProcessingTemplates

    # Generation data (added during workflow)
    current_profile_index: int
    current_demographic: Optional[DemographicAssignment]
    generated_content: Optional[BehavioralContent]
    generation_complete: bool

    # Output data (added during workflow)
    completed_profiles: List[SyntheticProfile]

    # Metadata (added during workflow)
    processing_errors: List[str]
    generation_stats: Dict[str, Any]
    assembly_complete: bool
    output_written: bool
    output_path: str

    # Internal state
    _llm_generator: Any


# ============================================================================
# DISTRIBUTION CALCULATION ENGINE
# ============================================================================


class DistributionCalculator:
    """Handles exact demographic distribution calculations."""

    @staticmethod
    def calculate_gender_distribution(
        total: int, proportions: Dict[str, int]
    ) -> Dict[str, int]:
        """Calculate exact gender distribution with rounding adjustment."""
        logger.info(f"Calculating gender distribution for {total} profiles")

        # Calculate base allocations
        allocations = {}
        remainders = {}

        for gender, percentage in proportions.items():
            exact_value = (percentage / 100.0) * total
            allocations[gender] = int(exact_value)
            remainders[gender] = exact_value - allocations[gender]

        # Distribute remaining profiles using largest remainder method
        total_allocated = sum(allocations.values())
        remaining = total - total_allocated

        if remaining > 0:
            # Sort by remainder descending
            sorted_remainders = sorted(
                remainders.items(), key=lambda x: x[1], reverse=True
            )
            for i in range(remaining):
                gender = sorted_remainders[i][0]
                allocations[gender] += 1

        logger.info(f"Gender distribution: {allocations}")
        return allocations

    @staticmethod
    def calculate_age_distribution(
        gender_dist: Dict[str, int], age_props: Dict[str, int]
    ) -> Dict[str, Dict[str, int]]:
        """Calculate age distribution within each gender group."""
        logger.info("Calculating age distribution per gender")

        result = {}
        for gender, gender_count in gender_dist.items():
            if gender_count == 0:
                result[gender] = {age_bucket: 0 for age_bucket in age_props.keys()}
                continue

            age_allocations = {}
            age_remainders = {}

            for age_bucket, percentage in age_props.items():
                exact_value = (percentage / 100.0) * gender_count
                age_allocations[age_bucket] = int(exact_value)
                age_remainders[age_bucket] = exact_value - age_allocations[age_bucket]

            # Distribute remaining using largest remainder method
            total_allocated = sum(age_allocations.values())
            remaining = gender_count - total_allocated

            if remaining > 0:
                sorted_remainders = sorted(
                    age_remainders.items(), key=lambda x: x[1], reverse=True
                )
                for i in range(remaining):
                    age_bucket = sorted_remainders[i][0]
                    age_allocations[age_bucket] += 1

            result[gender] = age_allocations

        logger.info(f"Age distribution: {result}")
        return result

    @staticmethod
    def calculate_ethnicity_distribution(
        age_gender_dist: Dict[str, Dict[str, int]], ethnicity_props: Dict[str, int]
    ) -> Dict[str, Dict[str, Dict[str, int]]]:
        """Calculate ethnicity distribution within each age-gender combination."""
        logger.info("Calculating ethnicity distribution per age-gender combination")

        result = {}
        for gender, age_dist in age_gender_dist.items():
            result[gender] = {}
            for age_bucket, age_count in age_dist.items():
                if age_count == 0:
                    result[gender][age_bucket] = {
                        ethnicity: 0 for ethnicity in ethnicity_props.keys()
                    }
                    continue

                ethnicity_allocations = {}
                ethnicity_remainders = {}

                for ethnicity, percentage in ethnicity_props.items():
                    exact_value = (percentage / 100.0) * age_count
                    ethnicity_allocations[ethnicity] = int(exact_value)
                    ethnicity_remainders[ethnicity] = (
                        exact_value - ethnicity_allocations[ethnicity]
                    )

                # Distribute remaining using largest remainder method
                total_allocated = sum(ethnicity_allocations.values())
                remaining = age_count - total_allocated

                if remaining > 0:
                    sorted_remainders = sorted(
                        ethnicity_remainders.items(), key=lambda x: x[1], reverse=True
                    )
                    for i in range(remaining):
                        ethnicity = sorted_remainders[i][0]
                        ethnicity_allocations[ethnicity] += 1

                result[gender][age_bucket] = ethnicity_allocations

        return result

    @staticmethod
    def generate_demographic_schedule(
        total: int,
        gender_props: Dict[str, int],
        age_props: Dict[str, int],
        ethnicity_props: Dict[str, int],
    ) -> List[DemographicAssignment]:
        """
        Generate demographic assignment schedule with EXACT quota compliance.

        FIXED VERSION: Uses simple direct assignment to ensure exact quotas.
        Previous hierarchical approach caused quota violations.
        """
        logger.info(f"Generating FIXED demographic schedule for {total} profiles")

        # Step 1: Calculate exact counts for each dimension using largest remainder method
        def calculate_exact_counts(
            proportions: Dict[str, int], total: int
        ) -> Dict[str, int]:
            allocations = {}
            remainders = {}

            for category, percentage in proportions.items():
                exact_value = (percentage / 100.0) * total
                allocations[category] = int(exact_value)
                remainders[category] = exact_value - allocations[category]

            # Distribute remaining using largest remainder method
            total_allocated = sum(allocations.values())
            remaining = total - total_allocated

            if remaining > 0:
                sorted_remainders = sorted(
                    remainders.items(), key=lambda x: x[1], reverse=True
                )
                for i in range(remaining):
                    category = sorted_remainders[i][0]
                    allocations[category] += 1

            return allocations

        gender_counts = calculate_exact_counts(gender_props, total)
        age_counts = calculate_exact_counts(age_props, total)
        ethnicity_counts = calculate_exact_counts(ethnicity_props, total)

        logger.info(f"Target gender distribution: {gender_counts}")
        logger.info(f"Target age distribution: {age_counts}")
        logger.info(f"Target ethnicity distribution: {ethnicity_counts}")

        # Step 2: Create assignment pools
        gender_pool = []
        for gender, count in gender_counts.items():
            gender_pool.extend([gender] * count)

        age_pool = []
        for age_bucket, count in age_counts.items():
            age_pool.extend([age_bucket] * count)

        ethnicity_pool = []
        for ethnicity, count in ethnicity_counts.items():
            ethnicity_pool.extend([ethnicity] * count)

        # Step 3: Shuffle pools to randomize assignments
        random.shuffle(gender_pool)
        random.shuffle(age_pool)
        random.shuffle(ethnicity_pool)

        # Step 4: Create assignments by combining pools
        schedule = []
        for i in range(total):
            assignment = DemographicAssignment(
                age_bucket=age_pool[i],
                gender=gender_pool[i],
                ethnicity=ethnicity_pool[i],
                profile_index=i,
            )
            schedule.append(assignment)

        # Step 5: Validate the schedule (ensure exact compliance)
        actual_gender = dict(Counter(assignment.gender for assignment in schedule))
        actual_age = dict(Counter(assignment.age_bucket for assignment in schedule))
        actual_ethnicity = dict(
            Counter(assignment.ethnicity for assignment in schedule)
        )

        # Validate by comparing counts for each category (order-independent)
        def validate_distribution(expected, actual, category_name):
            for category, expected_count in expected.items():
                actual_count = actual.get(category, 0)
                if actual_count != expected_count:
                    raise ValueError(
                        f"{category_name} distribution validation failed for {category}: "
                        f"expected {expected_count}, got {actual_count}"
                    )
            # Check for unexpected categories in actual
            for category in actual:
                if category not in expected:
                    raise ValueError(
                        f"{category_name} distribution validation failed: "
                        f"unexpected category {category} with count {actual[category]}"
                    )

        validate_distribution(gender_counts, actual_gender, "Gender")
        validate_distribution(age_counts, actual_age, "Age")
        validate_distribution(ethnicity_counts, actual_ethnicity, "Ethnicity")

        logger.info(
            f"✅ FIXED schedule validation passed - exact quota compliance achieved"
        )
        logger.info(f"Generated schedule with {len(schedule)} assignments")
        return schedule


# ============================================================================
# PERSONA CONTENT PROCESSOR
# ============================================================================


class PersonaProcessor:
    """Processes input personas to extract clean behavioral templates."""

    # Demographic content patterns to remove
    DEMOGRAPHIC_PATTERNS = [
        r"\b\d{1,2}[-\s]year[-\s]old\b",
        r"\bage\s+\d{1,2}\b",
        r"\b\d{1,2}\s+years?\s+old\b",
        r"\bmale\b|\bfemale\b",
        r"\bman\b|\bwoman\b|\bguy\b|\bgirl\b",
        r"\bhe\b|\bhis\b|\bhim\b|\bshe\b|\bher\b|\bhers\b",
        r"\bgender\b|\bsex\b",
        r"\bethnicity\b|\brace\b|\bnationality\b",
        r"\bwhite\b|\bblack\b|\basian\b|\bhispanic\b|\blatino\b|\blatina\b",
        r"\bcaucasian\b|\bafrican\b|\bamerican\b|\bindian\b|\bchinese\b|\bjapanese\b",
        r"\bmumbai\b|\bdelhi\b|\bbangalore\b|\bchennai\b|\bkolkata\b|\bhyderabad\b",
        r"\bindia\b|\bindian\b|\bhindi\b|\btamil\b|\btelugu\b|\bbengali\b",
        r"\brupees?\b|\b₹\b|\brs\.?\b|\binr\b",
        r"\bbandra\b|\bandheri\b|\bpowai\b|\bgoregaon\b|\bthane\b|\bnavi\s+mumbai\b",
    ]

    @staticmethod
    def clean_demographic_content(text: str) -> str:
        """Remove demographic references from text content."""
        cleaned_text = text

        for pattern in PersonaProcessor.DEMOGRAPHIC_PATTERNS:
            cleaned_text = re.sub(
                pattern, "[CONTENT]", cleaned_text, flags=re.IGNORECASE
            )

        # Clean up multiple spaces and placeholders
        cleaned_text = re.sub(r"\s+", " ", cleaned_text)
        cleaned_text = re.sub(r"\[CONTENT\]\s*", "", cleaned_text)
        cleaned_text = cleaned_text.strip()

        return cleaned_text

    @staticmethod
    def extract_behavioral_templates(
        personas: List[InputPersona],
    ) -> ProcessingTemplates:
        """Extract clean behavioral templates from personas."""
        logger.info(f"Extracting templates from {len(personas)} personas")

        about_templates = []
        goals_templates = []
        frustrations_templates = []
        need_state_templates = []
        occasions_templates = []

        for persona in personas:
            # Clean and extract about content
            cleaned_about = PersonaProcessor.clean_demographic_content(persona.about)
            if len(cleaned_about) > 50:  # Minimum content length
                about_templates.append(cleaned_about)

            # Clean and extract goals
            for goal in persona.goalsAndMotivations:
                cleaned_goal = PersonaProcessor.clean_demographic_content(goal)
                if len(cleaned_goal) > 20:
                    goals_templates.append(cleaned_goal)

            # Clean and extract frustrations
            for frustration in persona.frustrations:
                cleaned_frustration = PersonaProcessor.clean_demographic_content(
                    frustration
                )
                if len(cleaned_frustration) > 20:
                    frustrations_templates.append(cleaned_frustration)

            # Clean need state and occasions
            cleaned_need_state = PersonaProcessor.clean_demographic_content(
                persona.needState
            )
            if len(cleaned_need_state) > 10:
                need_state_templates.append(cleaned_need_state)

            cleaned_occasions = PersonaProcessor.clean_demographic_content(
                persona.occasions
            )
            if len(cleaned_occasions) > 20:
                occasions_templates.append(cleaned_occasions)

        templates = ProcessingTemplates(
            about_templates=list(set(about_templates)),  # Remove duplicates
            goals_templates=list(set(goals_templates)),
            frustrations_templates=list(set(frustrations_templates)),
            need_state_templates=list(set(need_state_templates)),
            occasions_templates=list(set(occasions_templates)),
        )

        logger.info(
            f"Extracted {len(templates.about_templates)} about templates, "
            f"{len(templates.goals_templates)} goals templates, "
            f"{len(templates.frustrations_templates)} frustrations templates"
        )

        return templates


# ============================================================================
# LLM CONTENT GENERATOR
# ============================================================================


class LLMContentGenerator:
    """Handles LLM integration for behavioral content generation."""

    def __init__(self):
        """Initialize the LLM with configuration."""
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY not found in environment variables")

        self.llm = ChatGoogleGenerativeAI(
            model=os.getenv("GEMINI_MODEL", "gemini-2.5-flash"),
            temperature=float(os.getenv("GEMINI_TEMPERATURE", "0.7")),
            max_tokens=int(os.getenv("GEMINI_MAX_TOKENS", "1500")),
            google_api_key=api_key,
        )

        self.max_retries = int(os.getenv("MAX_RETRIES", "5"))

        # Define generation prompt template
        self.prompt_template = PromptTemplate(
            input_variables=[
                "about_examples",
                "goals_examples",
                "frustrations_examples",
                "need_state_examples",
                "occasions_examples",
            ],
            template=self._get_generation_prompt(),
        )

    def _get_generation_prompt(self) -> str:
        """Get the LLM generation prompt template."""
        return """You are a behavioral content generator for synthetic audience profiles. 

CRITICAL REQUIREMENTS:
1. Generate ONLY behavioral content - NO demographic information whatsoever
2. Use ONLY gender-neutral language - avoid ALL pronouns (he, she, his, her, him, them, they)
3. Use neutral terms like "this person", "the individual", "someone", or "content creator"
4. Focus purely on behaviors, motivations, attitudes, and preferences
5. Return ONLY valid JSON with the exact structure specified
6. Content must be unique and realistic but avoid copying examples directly

ABSOLUTELY PROHIBITED CONTENT:
- ANY pronouns: he, she, his, her, him, them, they, their
- Gender words: male, female, man, woman, guy, girl, person (use "individual" instead)
- Age references: young, old, 23-year-old, millennial, gen-z, boomer
- Ethnicity/race references: Indian, Asian, White, Black, Hispanic, Latino
- Location references: Mumbai, Delhi, specific cities/countries, urban, rural
- Physical descriptions of any kind
- Names or specific personal identifiers

LANGUAGE GUIDELINES:
- Instead of "he creates content" → "the individual creates content"
- Instead of "she is passionate" → "this content creator is passionate"
- Instead of "they want to" → "the goal is to"
- Instead of "his work" → "the work"
- Use passive voice when needed to avoid pronouns

EXAMPLES FOR INSPIRATION (DO NOT COPY DIRECTLY):

About Examples:
{about_examples}

Goals Examples:
{goals_examples}

Frustrations Examples:
{frustrations_examples}

Need State Examples:
{need_state_examples}

Occasions Examples:
{occasions_examples}

Generate unique behavioral content inspired by these examples but with your own variation.

REQUIRED JSON OUTPUT FORMAT:
{{
    "about": "A behavioral description focusing on interests, lifestyle, and personality traits using gender-neutral language",
    "goalsAndMotivations": [
        "First goal or motivation without pronouns",
        "Second goal or motivation without pronouns", 
        "Third goal or motivation without pronouns"
    ],
    "frustrations": [
        "First frustration or challenge without pronouns",
        "Second frustration or challenge without pronouns",
        "Third frustration or challenge without pronouns"
    ],
    "needState": "Current emotional or motivational state",
    "occasions": "When and how content engagement happens, using neutral language"
}}

REMEMBER: NO PRONOUNS OR DEMOGRAPHIC REFERENCES AT ALL. Use "the individual", "this content creator", "someone" instead.

Generate the JSON now:"""

    def generate_content(self, templates: ProcessingTemplates) -> BehavioralContent:
        """Generate behavioral content using LLM."""
        # Prepare examples for prompt
        about_examples = "\n".join(templates.about_templates[:3])
        goals_examples = "\n".join(templates.goals_templates[:5])
        frustrations_examples = "\n".join(templates.frustrations_templates[:5])
        need_state_examples = "\n".join(templates.need_state_templates[:3])
        occasions_examples = "\n".join(templates.occasions_templates[:3])

        # Generate prompt
        prompt = self.prompt_template.format(
            about_examples=about_examples,
            goals_examples=goals_examples,
            frustrations_examples=frustrations_examples,
            need_state_examples=need_state_examples,
            occasions_examples=occasions_examples,
        )

        # Generate with retry logic
        for attempt in range(self.max_retries):
            try:
                response = self.llm.invoke(prompt)
                content_text = response.content

                # Parse JSON response
                content_data = self._parse_llm_response(content_text)

                # Clean and validate content
                cleaned_content = self._clean_generated_content(content_data)
                if self._validate_content(cleaned_content):
                    return BehavioralContent(**cleaned_content)
                else:
                    logger.warning(
                        f"Content validation failed on attempt {attempt + 1}"
                    )

            except Exception as e:
                logger.warning(f"Generation attempt {attempt + 1} failed: {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(2**attempt + random.uniform(0, 1))

        raise Exception(
            f"Failed to generate valid content after {self.max_retries} attempts"
        )

    def _parse_llm_response(self, response_text: str) -> Dict[str, Any]:
        """Parse LLM response to extract JSON."""
        try:
            # Try direct JSON parsing
            return json.loads(response_text)
        except json.JSONDecodeError:
            # Try to extract JSON from response
            json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            raise ValueError("No valid JSON found in response")

    def _clean_generated_content(self, content_data: Dict[str, Any]) -> Dict[str, Any]:
        """Clean generated content to remove demographic references."""
        cleaned_data = content_data.copy()

        # Define replacement patterns
        replacements = {
            # Pronouns
            r"\bhe\b": "the individual",
            r"\bshe\b": "the individual",
            r"\bhis\b": "the",
            r"\bher\b": "the",
            r"\bhim\b": "the individual",
            r"\bthey\b": "individuals",
            r"\bthem\b": "individuals",
            r"\btheir\b": "the",
            # Gender terms
            r"\bman\b": "individual",
            r"\bwoman\b": "individual",
            r"\bguy\b": "individual",
            r"\bgirl\b": "individual",
            r"\bperson\b": "individual",
            # Age terms
            r"\byoung\b": "",
            r"\bold\b": "experienced",
            r"\bmillennial\b": "individual",
            r"\bgen-z\b": "individual",
            r"\bboomer\b": "individual",
        }

        # Clean all text fields
        for field in ["about", "needState", "occasions"]:
            if field in cleaned_data and isinstance(cleaned_data[field], str):
                text = cleaned_data[field]
                for pattern, replacement in replacements.items():
                    text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
                # Clean up extra spaces
                text = re.sub(r"\s+", " ", text).strip()
                cleaned_data[field] = text

        # Clean list fields
        for field in ["goalsAndMotivations", "frustrations"]:
            if field in cleaned_data and isinstance(cleaned_data[field], list):
                cleaned_list = []
                for item in cleaned_data[field]:
                    if isinstance(item, str):
                        text = item
                        for pattern, replacement in replacements.items():
                            text = re.sub(
                                pattern, replacement, text, flags=re.IGNORECASE
                            )
                        # Clean up extra spaces
                        text = re.sub(r"\s+", " ", text).strip()
                        cleaned_list.append(text)
                    else:
                        cleaned_list.append(item)
                cleaned_data[field] = cleaned_list

        return cleaned_data

    def _validate_content(self, content_data: Dict[str, Any]) -> bool:
        """Validate generated content for demographic leakage."""
        # Check required fields
        required_fields = [
            "about",
            "goalsAndMotivations",
            "frustrations",
            "needState",
            "occasions",
        ]
        if not all(field in content_data for field in required_fields):
            return False

        # Check for demographic content
        all_text = json.dumps(content_data).lower()

        demographic_keywords = [
            # Gender pronouns and references
            "he",
            "she",
            "his",
            "her",
            "him",
            "them",
            "they",
            "their",
            "male",
            "female",
            "man",
            "woman",
            "guy",
            "girl",
            "person",
            # Age references
            "age",
            "year",
            "old",
            "young",
            "millennial",
            "gen-z",
            "genz",
            "boomer",
            # Ethnicity/race
            "ethnicity",
            "race",
            "white",
            "black",
            "asian",
            "hispanic",
            "latino",
            "latina",
            "caucasian",
            "african",
            "american",
            "indian",
            "chinese",
            "japanese",
            # Location references
            "mumbai",
            "delhi",
            "bangalore",
            "chennai",
            "kolkata",
            "hyderabad",
            "india",
            "urban",
            "rural",
            "city",
            "town",
            "village",
            # Currency/regional
            "rupee",
            "₹",
            "rs.",
            "inr",
        ]

        for keyword in demographic_keywords:
            # Use word boundaries to avoid false positives in compound words
            pattern = r"\b" + re.escape(keyword) + r"\b"
            if re.search(pattern, all_text, re.IGNORECASE):
                logger.warning(f"Demographic content detected: {keyword}")
                return False

        return True


# ============================================================================
# LANGGRAPH WORKFLOW NODES
# ============================================================================


def load_json_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Load and validate input JSON file."""
    logger.info("Loading and validating input JSON...")

    input_file = state["input_file"]
    if not input_file:
        raise ValueError("input_file not provided in state")

    with open(input_file, "r") as f:
        raw_data = json.load(f)

    request_data = RequestData(**raw_data)
    filter_details = request_data.get_filter_details()
    input_personas = request_data.get_personas()

    logger.info(f"Loaded request for {filter_details.user_req_responses} profiles")

    return {
        **state,
        "filter_details": filter_details,
        "input_personas": input_personas,
        "processing_errors": [],
        "generation_stats": {},
    }


def distribution_builder_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Generate demographic distribution schedule."""
    logger.info("Building demographic distribution schedule...")

    filter_details = state["filter_details"]

    demographic_schedule = DistributionCalculator.generate_demographic_schedule(
        filter_details.user_req_responses,
        filter_details.gender_proportions,
        filter_details.age_proportions,
        filter_details.ethnicity_proportions,
    )

    return {
        **state,
        "demographic_schedule": demographic_schedule,
        "current_profile_index": 0,
        "current_demographic": None,
        "generated_content": None,
        "completed_profiles": [],
    }


def persona_processor_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Extract behavioral templates from personas."""
    logger.info("Processing personas to extract behavioral templates...")

    input_personas = state["input_personas"]
    processing_templates = PersonaProcessor.extract_behavioral_templates(input_personas)

    return {
        **state,
        "processing_templates": processing_templates,
    }


def llm_generator_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Generate behavioral content for current demographic assignment."""
    demographic_schedule = state["demographic_schedule"]
    current_index = state["current_profile_index"]
    processing_templates = state["processing_templates"]

    # Check if generation is complete
    if current_index >= len(demographic_schedule):
        logger.info("All profiles generated successfully")
        return {
            **state,
            "generation_complete": True,
        }

    # Get current demographic assignment
    current_demographic = demographic_schedule[current_index]

    logger.info(
        f"Generating profile {current_index + 1}/{len(demographic_schedule)}: "
        f"{current_demographic.gender}, {current_demographic.age_bucket}, {current_demographic.ethnicity}"
    )

    # Initialize LLM generator if not exists
    if not hasattr(state, "_llm_generator"):
        state["_llm_generator"] = LLMContentGenerator()

    try:
        # Generate behavioral content
        behavioral_content = state["_llm_generator"].generate_content(
            processing_templates
        )

        # Create complete profile
        profile = SyntheticProfile(
            age_bucket=current_demographic.age_bucket,
            gender=current_demographic.gender,
            ethnicity=current_demographic.ethnicity,
            about=behavioral_content.about,
            goalsAndMotivations=behavioral_content.goalsAndMotivations,
            frustrations=behavioral_content.frustrations,
            needState=behavioral_content.needState,
            occasions=behavioral_content.occasions,
            profile_id=current_index + 1,
        )

        # Add to completed profiles
        completed_profiles = state.get("completed_profiles", [])
        completed_profiles.append(profile)

        return {
            **state,
            "current_demographic": current_demographic,
            "generated_content": behavioral_content,
            "completed_profiles": completed_profiles,
            "current_profile_index": current_index + 1,
        }

    except Exception as e:
        error_msg = f"Failed to generate profile {current_index + 1}: {str(e)}"
        logger.error(error_msg)

        processing_errors = state.get("processing_errors", [])
        processing_errors.append(error_msg)

        return {
            **state,
            "processing_errors": processing_errors,
            "generation_complete": True,  # Stop on error
        }


def profile_assembler_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Assemble and validate final audience profiles."""
    logger.info("Assembling and validating final profiles...")

    completed_profiles = state["completed_profiles"]
    filter_details = state["filter_details"]

    # Validate profile count
    expected_count = filter_details.user_req_responses
    actual_count = len(completed_profiles)

    if actual_count != expected_count:
        error_msg = (
            f"Profile count mismatch: expected {expected_count}, got {actual_count}"
        )
        logger.error(error_msg)
        raise ValueError(error_msg)

    # Validate distribution accuracy
    distribution_accuracy = SyntheticAudienceGenerator._validate_distribution_static(
        completed_profiles, filter_details
    )

    generation_stats = {
        "total_profiles": len(completed_profiles),
        "generation_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "distribution_accuracy": distribution_accuracy,
    }

    logger.info(f"Successfully assembled {len(completed_profiles)} profiles")

    return {
        **state,
        "generation_stats": generation_stats,
        "assembly_complete": True,
    }


def output_writer_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Write final audience to JSON file."""
    logger.info("Writing output to JSON file...")

    completed_profiles = state["completed_profiles"]
    generation_stats = state["generation_stats"]
    output_file = state.get("output_file", "synthetic_audience.json")
    input_file = state.get("input_file", "unknown")

    # Prepare output data
    output_data = {
        "synthetic_audience": [profile.model_dump() for profile in completed_profiles],
        "generation_metadata": {
            **generation_stats,
            "input_file": input_file,
        },
    }

    # Write to file
    with open(output_file, "w") as f:
        json.dump(output_data, f, indent=2)

    logger.info(f"Output written to {output_file}")

    return {
        **state,
        "output_written": True,
        "output_path": output_file,
    }


# ============================================================================
# LANGGRAPH WORKFLOW SETUP
# ============================================================================


def should_continue_generation(state: SyntheticAudienceState) -> str:
    """Determine if generation should continue or move to assembly."""
    if state.get("generation_complete", False):
        return "profile_assembler"
    return "llm_generator"


def create_workflow() -> StateGraph:
    """Create and configure the LangGraph workflow."""
    logger.info("Creating LangGraph workflow...")

    # Create the graph
    workflow = StateGraph(SyntheticAudienceState)

    # Add nodes
    workflow.add_node("load_json", load_json_node)
    workflow.add_node("distribution_builder", distribution_builder_node)
    workflow.add_node("persona_processor", persona_processor_node)
    workflow.add_node("llm_generator", llm_generator_node)
    workflow.add_node("profile_assembler", profile_assembler_node)
    workflow.add_node("output_writer", output_writer_node)

    # Define edges
    workflow.set_entry_point("load_json")
    workflow.add_edge("load_json", "distribution_builder")
    workflow.add_edge("distribution_builder", "persona_processor")
    workflow.add_edge("persona_processor", "llm_generator")

    # Conditional edge for LLM iteration
    workflow.add_conditional_edges(
        "llm_generator",
        should_continue_generation,
        {
            "llm_generator": "llm_generator",  # Continue generating
            "profile_assembler": "profile_assembler",  # Move to assembly
        },
    )

    workflow.add_edge("profile_assembler", "output_writer")
    workflow.add_edge("output_writer", END)

    return workflow


# ============================================================================
# MAIN APPLICATION CLASS
# ============================================================================


class SyntheticAudienceGenerator:
    """Main application class orchestrating the generation process."""

    def __init__(self):
        """Initialize the generator."""
        self.workflow = create_workflow()
        # Compile the workflow
        self.app = self.workflow.compile()
        logger.info("Synthetic Audience Generator initialized with LangGraph workflow")

    def process_request(self, input_file: str, output_file: str) -> Dict[str, Any]:
        """Process a complete generation request using LangGraph workflow."""
        logger.info(f"Processing request from {input_file} using LangGraph workflow")

        # Initial state
        initial_state = {
            "input_file": input_file,
            "output_file": output_file,
        }

        # Run the workflow
        try:
            final_state = None
            # Set high recursion limit for large batch processing
            config = {"recursion_limit": 1000}
            for step in self.app.stream(initial_state, config=config):
                node_name = list(step.keys())[0]
                final_state = step[node_name]

                # Progress tracking for LLM generation
                if (
                    node_name == "llm_generator"
                    and "current_profile_index" in final_state
                ):
                    current = final_state["current_profile_index"]
                    total = len(final_state.get("demographic_schedule", []))
                    if total > 0:
                        progress = (current / total) * 100
                        print(
                            f"Generation progress: {progress:.1f}% ({current}/{total})"
                        )

                logger.info(f"Completed node: {node_name}")

            # Check for errors
            if final_state and final_state.get("processing_errors"):
                raise Exception(
                    f"Processing errors: {final_state['processing_errors']}"
                )

            # Return generation metadata
            return final_state.get("generation_stats", {})

        except Exception as e:
            logger.error(f"Workflow execution failed: {str(e)}")
            raise

    @staticmethod
    def _validate_distribution_static(
        profiles: List[SyntheticProfile], filter_details: FilterDetails
    ) -> Dict[str, Any]:
        """Static method to validate distribution (for use in nodes)."""
        total_profiles = len(profiles)

        # Count actual distributions
        gender_counts = {}
        age_counts = {}
        ethnicity_counts = {}

        for profile in profiles:
            # Gender
            gender_counts[profile.gender] = gender_counts.get(profile.gender, 0) + 1
            # Age
            age_counts[profile.age_bucket] = age_counts.get(profile.age_bucket, 0) + 1
            # Ethnicity
            ethnicity_counts[profile.ethnicity] = (
                ethnicity_counts.get(profile.ethnicity, 0) + 1
            )

        # Calculate actual percentages
        gender_percentages = {
            k: round((v / total_profiles) * 100, 1) for k, v in gender_counts.items()
        }
        age_percentages = {
            k: round((v / total_profiles) * 100, 1) for k, v in age_counts.items()
        }
        ethnicity_percentages = {
            k: round((v / total_profiles) * 100, 1) for k, v in ethnicity_counts.items()
        }

        return {
            "total_profiles": total_profiles,
            "expected_total": filter_details.user_req_responses,
            "gender_distribution": {
                "expected": filter_details.gender_proportions,
                "actual_counts": gender_counts,
                "actual_percentages": gender_percentages,
            },
            "age_distribution": {
                "expected": filter_details.age_proportions,
                "actual_counts": age_counts,
                "actual_percentages": age_percentages,
            },
            "ethnicity_distribution": {
                "expected": filter_details.ethnicity_proportions,
                "actual_counts": ethnicity_counts,
                "actual_percentages": ethnicity_percentages,
            },
        }


# ============================================================================
# CLI INTERFACE
# ============================================================================


@click.command()
@click.option("--input", "-i", required=True, help="Input JSON file path")
@click.option("--output", "-o", required=True, help="Output JSON file path")
@click.option(
    "--validate-env", is_flag=True, help="Run environment validation before processing"
)
def main(input: str, output: str, validate_env: bool):
    """Synthetic Audience Generator MVP - Generate synthetic audience profiles."""

    if validate_env:
        logger.info("Running environment validation...")
        os.system("python validate_environment.py")

    try:
        generator = SyntheticAudienceGenerator()
        metadata = generator.process_request(input, output)

        print("\n" + "=" * 60)
        print("✅ GENERATION COMPLETED SUCCESSFULLY")
        print("=" * 60)
        print(f"Total Profiles Generated: {metadata['total_profiles']}")
        print(f"Expected Total: {metadata['distribution_accuracy']['expected_total']}")
        print(f"Output File: {output}")
        print(f"Generation Time: {metadata['generation_timestamp']}")

        # Print distribution summary
        print("\nDistribution Accuracy:")
        print(
            f"Gender: {metadata['distribution_accuracy']['gender_distribution']['actual_percentages']}"
        )
        print(
            f"Age: {metadata['distribution_accuracy']['age_distribution']['actual_percentages']}"
        )
        print(
            f"Ethnicity: {metadata['distribution_accuracy']['ethnicity_distribution']['actual_percentages']}"
        )

    except Exception as e:
        logger.error(f"Generation failed: {str(e)}")
        print(f"\n❌ GENERATION FAILED: {str(e)}")
        exit(1)


if __name__ == "__main__":
    main()
