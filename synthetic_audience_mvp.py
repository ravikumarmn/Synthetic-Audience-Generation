#!/usr/bin/env python3
"""
Synthetic Audience Generator MVP
Single-file implementation using LangChain + LangGraph + Gemini Flash 2.5

This MVP generates synthetic audience profiles with exact demographic distribution
matching while using LLM only for behavioral content generation.
"""

import json
import os
import re
import warnings
import time
import random
import asyncio
from typing import Dict, List, TypedDict, Optional, Tuple, Any
from dataclasses import dataclass
from collections import Counter
import logging
from concurrent.futures import ThreadPoolExecutor

# Core dependencies
from pydantic import BaseModel, Field, field_validator
from dotenv import load_dotenv
from tqdm import tqdm
import click

# Visualization dependencies
try:
    from IPython.display import Image, display

    IPYTHON_AVAILABLE = True
except ImportError:
    IPYTHON_AVAILABLE = False


# LangChain and LangGraph
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import AzureChatOpenAI
from langgraph.graph import StateGraph, END

# Import prompt
from prompt import BEHAVIORAL_CONTENT_PROMPT

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Parallel processing configuration
DEFAULT_BATCH_SIZE = int(os.getenv("PARALLEL_BATCH_SIZE", "5"))
DEFAULT_MAX_WORKERS = int(os.getenv("MAX_WORKERS", "3"))
DEFAULT_CONCURRENT_REQUESTS = int(os.getenv("CONCURRENT_REQUESTS", "3"))

# LLM Provider configuration
DEFAULT_LLM_PROVIDER = os.getenv("LLM_PROVIDER", "google")  # "google" or "azure"

# ============================================================================
# CORE DATA MODELS
# ============================================================================


class FilterDetails(BaseModel):
    """Input filter details from JSON request."""

    user_req_responses: int = Field(
        ..., gt=0, description="Number of responses required"
    )
    age_filter: Dict[str, int] = Field(..., description="Age bounds")
    age_proportions: Dict[str, int] = Field(..., description="Age group proportions")
    gender_filter: List[str] = Field(..., description="Allowed genders")
    gender_proportions: Dict[str, int] = Field(..., description="Gender proportions")
    ethnicity_filter: List[str] = Field(..., description="Allowed ethnicities")
    ethnicity_proportions: Dict[str, int] = Field(
        ..., description="Ethnicity proportions"
    )
    age_filter_complete: bool = Field(default=True)
    ethnicity_filter_complete: bool = Field(default=True)

    @field_validator("age_proportions", "gender_proportions", "ethnicity_proportions")
    @classmethod
    def validate_proportions_sum_to_100(cls, v):
        """Ensure proportions sum to 100."""
        total = sum(v.values())
        if total != 100:
            raise ValueError(f"Proportions must sum to 100, got {total}")
        return v


class InputPersona(BaseModel):
    """Input persona from JSON request."""

    id: int
    generatedTitle: str
    personaName: str
    age: int
    location: str
    ethnicity: str
    gender: str
    personaType: str
    about: str
    goalsAndMotivations: List[str]
    frustrations: List[str]
    needState: str
    occasions: str
    image: Optional[str] = None
    userId: Optional[int] = None
    createdAt: Optional[str] = None
    requestParams: Optional[Dict] = None


class RequestData(BaseModel):
    """Complete request data structure."""

    request: List[Dict[str, Any]]

    def get_filter_details(self) -> FilterDetails:
        """Extract filter details from request."""
        return FilterDetails(**self.request[0]["filter_details"])

    def get_personas(self) -> List[InputPersona]:
        """Extract personas from request."""
        return [InputPersona(**persona) for persona in self.request[0]["personas"]]


class DemographicAssignment(BaseModel):
    """Demographic assignment for a single profile."""

    age_bucket: str
    gender: str
    ethnicity: str
    profile_index: int


class BehavioralContent(BaseModel):
    """Generated behavioral content from LLM."""

    about: str = Field(..., description="About section")
    goalsAndMotivations: List[str] = Field(..., description="Goals and motivations")
    frustrations: List[str] = Field(..., description="Frustrations")
    needState: str = Field(..., description="Need state")
    occasions: str = Field(..., description="Occasions")


class SyntheticProfile(BaseModel):
    """Complete synthetic audience profile."""

    # Demographics (assigned by Python)
    age_bucket: str
    gender: str
    ethnicity: str

    # Behavioral content (generated by LLM)
    about: str
    goalsAndMotivations: List[str]
    frustrations: List[str]
    needState: str
    occasions: str

    # Metadata
    profile_id: int


class ProcessingTemplates(BaseModel):
    """Cleaned templates for LLM generation."""

    about_templates: List[str]
    goals_templates: List[str]
    frustrations_templates: List[str]
    need_state_templates: List[str]
    occasions_templates: List[str]


# ============================================================================
# LANGGRAPH STATE MANAGEMENT
# ============================================================================


class SyntheticAudienceState(TypedDict, total=False):
    """State management for LangGraph workflow."""

    # Input data (required)
    input_file: str
    output_file: str

    # Parsed data (added during workflow)
    filter_details: FilterDetails
    input_personas: List[InputPersona]

    # Processing data (added during workflow)
    demographic_schedule: List[DemographicAssignment]
    processing_templates: ProcessingTemplates

    # Generation data (added during workflow)
    current_profile_index: int
    current_demographic: Optional[DemographicAssignment]
    generated_content: Optional[BehavioralContent]
    generation_complete: bool

    # Output data (added during workflow)
    completed_profiles: List[SyntheticProfile]

    # Metadata (added during workflow)
    processing_errors: List[str]
    generation_stats: Dict[str, Any]
    assembly_complete: bool
    output_written: bool
    output_path: str

    # Internal state
    _llm_generator: Any


# ============================================================================
# DISTRIBUTION CALCULATION ENGINE
# ============================================================================


class DistributionCalculator:
    """Handles exact demographic distribution calculations."""

    @staticmethod
    def calculate_gender_distribution(
        total: int, proportions: Dict[str, int]
    ) -> Dict[str, int]:
        """Calculate exact gender distribution with rounding adjustment."""
        logger.info(f"Calculating gender distribution for {total} profiles")

        # Calculate base allocations
        allocations = {}
        remainders = {}

        for gender, percentage in proportions.items():
            exact_value = (percentage / 100.0) * total
            allocations[gender] = int(exact_value)
            remainders[gender] = exact_value - allocations[gender]

        # Distribute remaining profiles using largest remainder method
        total_allocated = sum(allocations.values())
        remaining = total - total_allocated

        if remaining > 0:
            # Sort by remainder descending
            sorted_remainders = sorted(
                remainders.items(), key=lambda x: x[1], reverse=True
            )
            for i in range(remaining):
                gender = sorted_remainders[i][0]
                allocations[gender] += 1

        logger.info(f"Gender distribution: {allocations}")
        return allocations

    @staticmethod
    def calculate_age_distribution(
        gender_dist: Dict[str, int], age_props: Dict[str, int]
    ) -> Dict[str, Dict[str, int]]:
        """Calculate age distribution within each gender group."""
        logger.info("Calculating age distribution per gender")

        result = {}
        for gender, gender_count in gender_dist.items():
            if gender_count == 0:
                result[gender] = {age_bucket: 0 for age_bucket in age_props.keys()}
                continue

            age_allocations = {}
            age_remainders = {}

            for age_bucket, percentage in age_props.items():
                exact_value = (percentage / 100.0) * gender_count
                age_allocations[age_bucket] = int(exact_value)
                age_remainders[age_bucket] = exact_value - age_allocations[age_bucket]

            # Distribute remaining using largest remainder method
            total_allocated = sum(age_allocations.values())
            remaining = gender_count - total_allocated

            if remaining > 0:
                sorted_remainders = sorted(
                    age_remainders.items(), key=lambda x: x[1], reverse=True
                )
                for i in range(remaining):
                    age_bucket = sorted_remainders[i][0]
                    age_allocations[age_bucket] += 1

            result[gender] = age_allocations

        logger.info(f"Age distribution: {result}")
        return result

    @staticmethod
    def calculate_ethnicity_distribution(
        age_gender_dist: Dict[str, Dict[str, int]], ethnicity_props: Dict[str, int]
    ) -> Dict[str, Dict[str, Dict[str, int]]]:
        """Calculate ethnicity distribution within each age-gender combination."""
        logger.info("Calculating ethnicity distribution per age-gender combination")

        result = {}
        for gender, age_dist in age_gender_dist.items():
            result[gender] = {}
            for age_bucket, age_count in age_dist.items():
                if age_count == 0:
                    result[gender][age_bucket] = {
                        ethnicity: 0 for ethnicity in ethnicity_props.keys()
                    }
                    continue

                ethnicity_allocations = {}
                ethnicity_remainders = {}

                for ethnicity, percentage in ethnicity_props.items():
                    exact_value = (percentage / 100.0) * age_count
                    ethnicity_allocations[ethnicity] = int(exact_value)
                    ethnicity_remainders[ethnicity] = (
                        exact_value - ethnicity_allocations[ethnicity]
                    )

                # Distribute remaining using largest remainder method
                total_allocated = sum(ethnicity_allocations.values())
                remaining = age_count - total_allocated

                if remaining > 0:
                    sorted_remainders = sorted(
                        ethnicity_remainders.items(), key=lambda x: x[1], reverse=True
                    )
                    for i in range(remaining):
                        ethnicity = sorted_remainders[i][0]
                        ethnicity_allocations[ethnicity] += 1

                result[gender][age_bucket] = ethnicity_allocations

        return result

    @staticmethod
    def generate_demographic_schedule(
        total: int,
        gender_props: Dict[str, int],
        age_props: Dict[str, int],
        ethnicity_props: Dict[str, int],
    ) -> List[DemographicAssignment]:
        """
        Generate demographic assignment schedule with EXACT quota compliance.

        FIXED VERSION: Uses simple direct assignment to ensure exact quotas.
        Previous hierarchical approach caused quota violations.
        """
        logger.info(f"Generating FIXED demographic schedule for {total} profiles")

        # Step 1: Calculate exact counts for each dimension using largest remainder method
        def calculate_exact_counts(
            proportions: Dict[str, int], total: int
        ) -> Dict[str, int]:
            allocations = {}
            remainders = {}

            for category, percentage in proportions.items():
                exact_value = (percentage / 100.0) * total
                allocations[category] = int(exact_value)
                remainders[category] = exact_value - allocations[category]

            # Distribute remaining using largest remainder method
            total_allocated = sum(allocations.values())
            remaining = total - total_allocated

            if remaining > 0:
                sorted_remainders = sorted(
                    remainders.items(), key=lambda x: x[1], reverse=True
                )
                for i in range(remaining):
                    category = sorted_remainders[i][0]
                    allocations[category] += 1

            return allocations

        gender_counts = calculate_exact_counts(gender_props, total)
        age_counts = calculate_exact_counts(age_props, total)
        ethnicity_counts = calculate_exact_counts(ethnicity_props, total)

        logger.info(f"Target gender distribution: {gender_counts}")
        logger.info(f"Target age distribution: {age_counts}")
        logger.info(f"Target ethnicity distribution: {ethnicity_counts}")

        # Step 2: Create assignment pools
        gender_pool = []
        for gender, count in gender_counts.items():
            gender_pool.extend([gender] * count)

        age_pool = []
        for age_bucket, count in age_counts.items():
            age_pool.extend([age_bucket] * count)

        ethnicity_pool = []
        for ethnicity, count in ethnicity_counts.items():
            ethnicity_pool.extend([ethnicity] * count)

        # Step 3: Shuffle pools to randomize assignments
        random.shuffle(gender_pool)
        random.shuffle(age_pool)
        random.shuffle(ethnicity_pool)

        # Step 4: Create assignments by combining pools
        schedule = []
        for i in range(total):
            assignment = DemographicAssignment(
                age_bucket=age_pool[i],
                gender=gender_pool[i],
                ethnicity=ethnicity_pool[i],
                profile_index=i,
            )
            schedule.append(assignment)

        # Step 5: Validate the schedule (ensure exact compliance)
        actual_gender = dict(Counter(assignment.gender for assignment in schedule))
        actual_age = dict(Counter(assignment.age_bucket for assignment in schedule))
        actual_ethnicity = dict(
            Counter(assignment.ethnicity for assignment in schedule)
        )

        # Validate by comparing counts for each category (order-independent)
        def validate_distribution(expected, actual, category_name):
            for category, expected_count in expected.items():
                actual_count = actual.get(category, 0)
                if actual_count != expected_count:
                    raise ValueError(
                        f"{category_name} distribution validation failed for {category}: "
                        f"expected {expected_count}, got {actual_count}"
                    )
            # Check for unexpected categories in actual
            for category in actual:
                if category not in expected:
                    raise ValueError(
                        f"{category_name} distribution validation failed: "
                        f"unexpected category {category} with count {actual[category]}"
                    )

        validate_distribution(gender_counts, actual_gender, "Gender")
        validate_distribution(age_counts, actual_age, "Age")
        validate_distribution(ethnicity_counts, actual_ethnicity, "Ethnicity")

        logger.info(
            f"âœ… FIXED schedule validation passed - exact quota compliance achieved"
        )
        logger.info(f"Generated schedule with {len(schedule)} assignments")
        return schedule


# ============================================================================
# PERSONA CONTENT PROCESSOR
# ============================================================================


class PersonaProcessor:
    """Processes input personas to extract behavioral templates."""

    @staticmethod
    def extract_behavioral_templates(
        personas: List[InputPersona],
    ) -> ProcessingTemplates:
        """Extract clean behavioral templates from personas."""
        logger.info(f"Extracting templates from {len(personas)} personas")

        about_templates = []
        goals_templates = []
        frustrations_templates = []
        need_state_templates = []
        occasions_templates = []

        for persona in personas:
            # Extract content directly - let the prompt handle demographic filtering
            if persona.about and len(persona.about.strip()) > 10:
                about_templates.append(persona.about.strip())

            # Extract goals
            for goal in persona.goalsAndMotivations:
                if goal and len(goal.strip()) > 5:
                    goals_templates.append(goal.strip())

            # Extract frustrations
            for frustration in persona.frustrations:
                if frustration and len(frustration.strip()) > 5:
                    frustrations_templates.append(frustration.strip())

            # Extract need state and occasions
            if persona.needState and len(persona.needState.strip()) > 5:
                need_state_templates.append(persona.needState.strip())

            if persona.occasions and len(persona.occasions.strip()) > 5:
                occasions_templates.append(persona.occasions.strip())

        templates = ProcessingTemplates(
            about_templates=list(set(about_templates)),  # Remove duplicates
            goals_templates=list(set(goals_templates)),
            frustrations_templates=list(set(frustrations_templates)),
            need_state_templates=list(set(need_state_templates)),
            occasions_templates=list(set(occasions_templates)),
        )

        logger.info(
            f"Extracted {len(templates.about_templates)} about templates, "
            f"{len(templates.goals_templates)} goals templates, "
            f"{len(templates.frustrations_templates)} frustrations templates"
        )

        return templates


# ============================================================================
# LLM CONTENT GENERATOR
# ============================================================================


class LLMContentGenerator:
    """Handles LLM integration for behavioral content generation."""

    def __init__(self, provider: str = None):
        """Initialize the LLM with configuration."""
        self.provider = provider or os.getenv("LLM_PROVIDER", "google")

        if self.provider == "azure":
            # Azure OpenAI configuration
            azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
            api_key = os.getenv("AZURE_OPENAI_API_KEY")
            deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
            api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")

            if not all([azure_endpoint, api_key, deployment_name]):
                raise ValueError(
                    "Azure OpenAI configuration missing. Required: "
                    "AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME"
                )

            self.llm = AzureChatOpenAI(
                azure_endpoint=azure_endpoint,
                api_key=api_key,
                azure_deployment=deployment_name,
                api_version=api_version,
                temperature=float(os.getenv("AZURE_OPENAI_TEMPERATURE", "0.7")),
                max_tokens=int(os.getenv("AZURE_OPENAI_MAX_TOKENS", "1500")),
            )
        else:
            # Google Gemini configuration (default)
            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY not found in environment variables")

            self.llm = ChatGoogleGenerativeAI(
                model=os.getenv("GEMINI_MODEL", "gemini-2.5-flash"),
                temperature=float(os.getenv("GEMINI_TEMPERATURE", "0.7")),
                max_tokens=int(os.getenv("GEMINI_MAX_TOKENS", "1500")),
                google_api_key=api_key,
            )

        self.max_retries = int(os.getenv("MAX_RETRIES", "5"))

        # Use the imported prompt template
        self.prompt_template = PromptTemplate(
            input_variables=[
                "about_examples",
                "goals_examples",
                "frustrations_examples",
                "need_state_examples",
                "occasions_examples",
            ],
            template=BEHAVIORAL_CONTENT_PROMPT,
        )

    def generate_content(self, templates: ProcessingTemplates) -> BehavioralContent:
        """Generate behavioral content using LLM."""
        # Prepare examples for prompt (configurable limits)
        max_examples = int(os.getenv("MAX_PROMPT_EXAMPLES", "3"))
        about_examples = "\n".join(templates.about_templates[:max_examples])
        goals_examples = "\n".join(templates.goals_templates[: max_examples * 2])
        frustrations_examples = "\n".join(
            templates.frustrations_templates[: max_examples * 2]
        )
        need_state_examples = "\n".join(templates.need_state_templates[:max_examples])
        occasions_examples = "\n".join(templates.occasions_templates[:max_examples])

        # Generate prompt
        prompt = self.prompt_template.format(
            about_examples=about_examples,
            goals_examples=goals_examples,
            frustrations_examples=frustrations_examples,
            need_state_examples=need_state_examples,
            occasions_examples=occasions_examples,
        )

        # Generate with retry logic
        for attempt in range(self.max_retries):
            try:
                response = self.llm.invoke(prompt)
                content_text = response.content

                # Parse JSON response
                content_data = self._parse_llm_response(content_text)

                # Basic validation - just check required fields exist
                if self._validate_basic_structure(content_data):
                    return BehavioralContent(**content_data)
                else:
                    logger.warning(
                        f"Content structure validation failed on attempt {attempt + 1}"
                    )

            except Exception as e:
                logger.warning(f"Generation attempt {attempt + 1} failed: {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(2**attempt + random.uniform(0, 1))

        raise Exception(
            f"Failed to generate valid content after {self.max_retries} attempts"
        )

    def _parse_llm_response(self, response_text: str) -> Dict[str, Any]:
        """Parse LLM response to extract JSON."""
        try:
            # Try direct JSON parsing
            return json.loads(response_text)
        except json.JSONDecodeError:
            # Try to extract JSON from response
            json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            raise ValueError("No valid JSON found in response")

    def _validate_basic_structure(self, content_data: Dict[str, Any]) -> bool:
        """Basic validation to ensure required fields exist."""
        required_fields = [
            "about",
            "goalsAndMotivations",
            "frustrations",
            "needState",
            "occasions",
        ]

        # Check all required fields exist
        if not all(field in content_data for field in required_fields):
            logger.warning(f"Missing required fields. Expected: {required_fields}")
            return False

        # Check list fields are actually lists
        list_fields = ["goalsAndMotivations", "frustrations"]
        for field in list_fields:
            if (
                not isinstance(content_data[field], list)
                or len(content_data[field]) == 0
            ):
                logger.warning(f"Field {field} must be a non-empty list")
                return False

        # Check string fields are not empty
        string_fields = ["about", "needState", "occasions"]
        for field in string_fields:
            if (
                not isinstance(content_data[field], str)
                or len(content_data[field].strip()) == 0
            ):
                logger.warning(f"Field {field} must be a non-empty string")
                return False

        return True


class AzureOpenAIContentGenerator:
    """Handles Azure OpenAI integration for behavioral content generation."""

    def __init__(self):
        """Initialize the Azure OpenAI LLM with configuration."""
        # Azure OpenAI configuration
        azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        api_key = os.getenv("AZURE_OPENAI_API_KEY")
        deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
        api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")

        if not all([azure_endpoint, api_key, deployment_name]):
            raise ValueError(
                "Azure OpenAI configuration missing. Required: "
                "AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME"
            )

        self.llm = AzureChatOpenAI(
            azure_endpoint=azure_endpoint,
            api_key=api_key,
            azure_deployment=deployment_name,
            api_version=api_version,
            temperature=float(os.getenv("AZURE_OPENAI_TEMPERATURE", "0.7")),
            max_tokens=int(os.getenv("AZURE_OPENAI_MAX_TOKENS", "1500")),
        )

        self.max_retries = int(os.getenv("MAX_RETRIES", "5"))

        # Use the imported prompt template
        self.prompt_template = PromptTemplate(
            input_variables=[
                "about_examples",
                "goals_examples",
                "frustrations_examples",
                "need_state_examples",
                "occasions_examples",
            ],
            template=BEHAVIORAL_CONTENT_PROMPT,
        )

    def generate_content(self, templates: ProcessingTemplates) -> BehavioralContent:
        """Generate behavioral content using Azure OpenAI."""
        # Prepare examples for prompt (configurable limits)
        max_examples = int(os.getenv("MAX_PROMPT_EXAMPLES", "3"))
        about_examples = "\n".join(templates.about_templates[:max_examples])
        goals_examples = "\n".join(templates.goals_templates[: max_examples * 2])
        frustrations_examples = "\n".join(
            templates.frustrations_templates[: max_examples * 2]
        )
        need_state_examples = "\n".join(templates.need_state_templates[:max_examples])
        occasions_examples = "\n".join(templates.occasions_templates[:max_examples])

        # Generate prompt
        prompt = self.prompt_template.format(
            about_examples=about_examples,
            goals_examples=goals_examples,
            frustrations_examples=frustrations_examples,
            need_state_examples=need_state_examples,
            occasions_examples=occasions_examples,
        )

        # Generate with retry logic
        for attempt in range(self.max_retries):
            try:
                response = self.llm.invoke(prompt)
                content_text = response.content

                # Parse JSON response
                content_data = self._parse_llm_response(content_text)

                # Basic validation - just check required fields exist
                if self._validate_basic_structure(content_data):
                    return BehavioralContent(**content_data)
                else:
                    logger.warning(
                        f"Content structure validation failed on attempt {attempt + 1}"
                    )

            except Exception as e:
                logger.warning(f"Generation attempt {attempt + 1} failed: {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(2**attempt + random.uniform(0, 1))

        raise Exception(
            f"Failed to generate valid content after {self.max_retries} attempts"
        )

    def _parse_llm_response(self, response_text: str) -> Dict[str, Any]:
        """Parse LLM response to extract JSON."""
        try:
            # Try direct JSON parsing
            return json.loads(response_text)
        except json.JSONDecodeError:
            # Try to extract JSON from response
            json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            raise ValueError("No valid JSON found in response")

    def _validate_basic_structure(self, content_data: Dict[str, Any]) -> bool:
        """Basic validation to ensure required fields exist."""
        required_fields = [
            "about",
            "goalsAndMotivations",
            "frustrations",
            "needState",
            "occasions",
        ]

        # Check all required fields exist
        if not all(field in content_data for field in required_fields):
            logger.warning(f"Missing required fields. Expected: {required_fields}")
            return False

        # Check list fields are actually lists
        list_fields = ["goalsAndMotivations", "frustrations"]
        for field in list_fields:
            if (
                not isinstance(content_data[field], list)
                or len(content_data[field]) == 0
            ):
                logger.warning(f"Field {field} must be a non-empty list")
                return False

        # Check string fields are not empty
        string_fields = ["about", "needState", "occasions"]
        for field in string_fields:
            if (
                not isinstance(content_data[field], str)
                or len(content_data[field].strip()) == 0
            ):
                logger.warning(f"Field {field} must be a non-empty string")
                return False

        return True


class AsyncLLMContentGenerator:
    """Async version of LLM content generator for parallel processing."""

    def __init__(self, provider: str = None):
        """Initialize the async LLM generator."""
        self.provider = provider or os.getenv("LLM_PROVIDER", "google")

        if self.provider == "azure":
            # Azure OpenAI configuration
            azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
            api_key = os.getenv("AZURE_OPENAI_API_KEY")
            deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
            api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")

            if not all([azure_endpoint, api_key, deployment_name]):
                raise ValueError(
                    "Azure OpenAI configuration missing. Required: "
                    "AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_DEPLOYMENT_NAME"
                )

            self.llm = AzureChatOpenAI(
                azure_endpoint=azure_endpoint,
                api_key=api_key,
                azure_deployment=deployment_name,
                api_version=api_version,
                temperature=float(os.getenv("AZURE_OPENAI_TEMPERATURE", "0.7")),
                max_tokens=int(os.getenv("AZURE_OPENAI_MAX_TOKENS", "1500")),
            )
        else:
            # Google Gemini configuration (default)
            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                raise ValueError("GOOGLE_API_KEY not found in environment variables")

            self.llm = ChatGoogleGenerativeAI(
                model=os.getenv("GEMINI_MODEL", "gemini-2.5-flash"),
                temperature=float(os.getenv("GEMINI_TEMPERATURE", "0.7")),
                max_tokens=int(os.getenv("GEMINI_MAX_TOKENS", "1500")),
                google_api_key=api_key,
            )

        self.max_retries = int(os.getenv("MAX_RETRIES", "5"))
        self.prompt_template = PromptTemplate(
            input_variables=[
                "about_examples",
                "goals_examples",
                "frustrations_examples",
                "need_state_examples",
                "occasions_examples",
            ],
            template=BEHAVIORAL_CONTENT_PROMPT,
        )

    async def generate_content_async(
        self, templates: ProcessingTemplates, demographic: DemographicAssignment
    ) -> Tuple[DemographicAssignment, BehavioralContent]:
        """Generate behavioral content asynchronously."""
        loop = asyncio.get_event_loop()

        # Run the synchronous LLM call in a thread pool
        with ThreadPoolExecutor(max_workers=1) as executor:
            content = await loop.run_in_executor(
                executor, self._generate_content_sync, templates
            )

        return demographic, content

    def _generate_content_sync(
        self, templates: ProcessingTemplates
    ) -> BehavioralContent:
        """Synchronous content generation (runs in thread pool)."""
        # Prepare examples for prompt
        max_examples = int(os.getenv("MAX_PROMPT_EXAMPLES", "3"))
        about_examples = "\n".join(templates.about_templates[:max_examples])
        goals_examples = "\n".join(templates.goals_templates[: max_examples * 2])
        frustrations_examples = "\n".join(
            templates.frustrations_templates[: max_examples * 2]
        )
        need_state_examples = "\n".join(templates.need_state_templates[:max_examples])
        occasions_examples = "\n".join(templates.occasions_templates[:max_examples])

        # Generate prompt
        prompt = self.prompt_template.format(
            about_examples=about_examples,
            goals_examples=goals_examples,
            frustrations_examples=frustrations_examples,
            need_state_examples=need_state_examples,
            occasions_examples=occasions_examples,
        )

        # Generate with retry logic
        for attempt in range(self.max_retries):
            try:
                response = self.llm.invoke(prompt)
                content_text = response.content

                # Parse JSON response
                content_data = self._parse_llm_response(content_text)

                # Basic validation
                if self._validate_basic_structure(content_data):
                    return BehavioralContent(**content_data)
                else:
                    logger.warning(
                        f"Content structure validation failed on attempt {attempt + 1}"
                    )

            except Exception as e:
                logger.warning(f"Generation attempt {attempt + 1} failed: {str(e)}")
                if attempt < self.max_retries - 1:
                    time.sleep(2**attempt + random.uniform(0, 1))

        raise Exception(
            f"Failed to generate valid content after {self.max_retries} attempts"
        )

    def _parse_llm_response(self, response_text: str) -> Dict[str, Any]:
        """Parse LLM response to extract JSON."""
        try:
            return json.loads(response_text)
        except json.JSONDecodeError:
            json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            raise ValueError("No valid JSON found in response")

    def _validate_basic_structure(self, content_data: Dict[str, Any]) -> bool:
        """Basic validation to ensure required fields exist."""
        required_fields = [
            "about",
            "goalsAndMotivations",
            "frustrations",
            "needState",
            "occasions",
        ]

        if not all(field in content_data for field in required_fields):
            return False

        list_fields = ["goalsAndMotivations", "frustrations"]
        for field in list_fields:
            if (
                not isinstance(content_data[field], list)
                or len(content_data[field]) == 0
            ):
                return False

        string_fields = ["about", "needState", "occasions"]
        for field in string_fields:
            if (
                not isinstance(content_data[field], str)
                or len(content_data[field].strip()) == 0
            ):
                return False

        return True


# ============================================================================
# PARALLEL BATCH PROCESSOR
# ============================================================================


class ParallelBatchProcessor:
    """Handles parallel batch processing of profile generation."""

    def __init__(
        self,
        batch_size: int = DEFAULT_BATCH_SIZE,
        max_workers: int = DEFAULT_MAX_WORKERS,
        provider: str = None,
    ):
        """Initialize the batch processor."""
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.provider = provider or os.getenv("LLM_PROVIDER", "google")
        self.async_generator = AsyncLLMContentGenerator(provider=self.provider)

    async def process_batch_async(
        self,
        demographic_batch: List[DemographicAssignment],
        templates: ProcessingTemplates,
    ) -> List[SyntheticProfile]:
        """Process a batch of demographics in parallel."""
        logger.info(
            f"Processing batch of {len(demographic_batch)} profiles in parallel"
        )

        # Create semaphore to limit concurrent requests
        semaphore = asyncio.Semaphore(DEFAULT_CONCURRENT_REQUESTS)

        async def generate_with_semaphore(demographic: DemographicAssignment):
            async with semaphore:
                return await self.async_generator.generate_content_async(
                    templates, demographic
                )

        # Execute all generations in parallel
        tasks = [generate_with_semaphore(demo) for demo in demographic_batch]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results and create profiles
        profiles = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Failed to generate profile {i+1}: {str(result)}")
                continue

            demographic, behavioral_content = result
            profile = SyntheticProfile(
                age_bucket=demographic.age_bucket,
                gender=demographic.gender,
                ethnicity=demographic.ethnicity,
                about=behavioral_content.about,
                goalsAndMotivations=behavioral_content.goalsAndMotivations,
                frustrations=behavioral_content.frustrations,
                needState=behavioral_content.needState,
                occasions=behavioral_content.occasions,
                profile_id=demographic.profile_index + 1,
            )
            profiles.append(profile)

        logger.info(f"Successfully generated {len(profiles)} profiles in batch")
        return profiles

    def process_all_batches(
        self,
        demographic_schedule: List[DemographicAssignment],
        templates: ProcessingTemplates,
    ) -> List[SyntheticProfile]:
        """Process all demographics in batches."""
        logger.info(
            f"Processing {len(demographic_schedule)} profiles in batches of {self.batch_size}"
        )

        all_profiles = []

        # Split into batches
        for i in range(0, len(demographic_schedule), self.batch_size):
            batch = demographic_schedule[i : i + self.batch_size]
            batch_num = (i // self.batch_size) + 1
            total_batches = (
                len(demographic_schedule) + self.batch_size - 1
            ) // self.batch_size

            logger.info(f"Processing batch {batch_num}/{total_batches}")

            # Run async batch processing
            batch_profiles = asyncio.run(self.process_batch_async(batch, templates))
            all_profiles.extend(batch_profiles)

            # Progress update
            progress = len(all_profiles) / len(demographic_schedule) * 100
            logger.info(
                f"Overall progress: {progress:.1f}% ({len(all_profiles)}/{len(demographic_schedule)})"
            )

        return all_profiles


# ============================================================================
# LANGGRAPH WORKFLOW NODES
# ============================================================================


def load_json_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Load and validate input JSON file."""
    logger.info("Loading and validating input JSON...")

    input_file = state["input_file"]
    if not input_file:
        raise ValueError("input_file not provided in state")

    with open(input_file, "r") as f:
        raw_data = json.load(f)

    request_data = RequestData(**raw_data)
    filter_details = request_data.get_filter_details()
    input_personas = request_data.get_personas()

    logger.info(f"Loaded request for {filter_details.user_req_responses} profiles")

    return {
        **state,
        "filter_details": filter_details,
        "input_personas": input_personas,
        "processing_errors": [],
        "generation_stats": {},
    }


def distribution_builder_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Generate demographic distribution schedule."""
    logger.info("Building demographic distribution schedule...")

    filter_details = state["filter_details"]

    demographic_schedule = DistributionCalculator.generate_demographic_schedule(
        filter_details.user_req_responses,
        filter_details.gender_proportions,
        filter_details.age_proportions,
        filter_details.ethnicity_proportions,
    )

    return {
        **state,
        "demographic_schedule": demographic_schedule,
        "current_profile_index": 0,
        "current_demographic": None,
        "generated_content": None,
        "completed_profiles": [],
    }


def persona_processor_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Extract behavioral templates from personas."""
    logger.info("Processing personas to extract behavioral templates...")

    input_personas = state["input_personas"]
    processing_templates = PersonaProcessor.extract_behavioral_templates(input_personas)

    return {
        **state,
        "processing_templates": processing_templates,
    }


def llm_generator_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Generate behavioral content for current demographic assignment."""
    demographic_schedule = state["demographic_schedule"]
    current_index = state["current_profile_index"]
    processing_templates = state["processing_templates"]

    # Check if generation is complete
    if current_index >= len(demographic_schedule):
        logger.info("All profiles generated successfully")
        return {
            **state,
            "generation_complete": True,
        }

    # Get current demographic assignment
    current_demographic = demographic_schedule[current_index]

    logger.info(
        f"Generating profile {current_index + 1}/{len(demographic_schedule)}: "
        f"{current_demographic.gender}, {current_demographic.age_bucket}, {current_demographic.ethnicity}"
    )

    # Initialize LLM generator if not exists
    if not hasattr(state, "_llm_generator"):
        provider = os.getenv("LLM_PROVIDER", "google")
        state["_llm_generator"] = LLMContentGenerator(provider=provider)

    try:
        # Generate behavioral content
        behavioral_content = state["_llm_generator"].generate_content(
            processing_templates
        )

        # Create complete profile
        profile = SyntheticProfile(
            age_bucket=current_demographic.age_bucket,
            gender=current_demographic.gender,
            ethnicity=current_demographic.ethnicity,
            about=behavioral_content.about,
            goalsAndMotivations=behavioral_content.goalsAndMotivations,
            frustrations=behavioral_content.frustrations,
            needState=behavioral_content.needState,
            occasions=behavioral_content.occasions,
            profile_id=current_index + 1,
        )

        # Add to completed profiles
        completed_profiles = state.get("completed_profiles", [])
        completed_profiles.append(profile)

        return {
            **state,
            "current_demographic": current_demographic,
            "generated_content": behavioral_content,
            "completed_profiles": completed_profiles,
            "current_profile_index": current_index + 1,
        }

    except Exception as e:
        error_msg = f"Failed to generate profile {current_index + 1}: {str(e)}"
        logger.error(error_msg)

        processing_errors = state.get("processing_errors", [])
        processing_errors.append(error_msg)

        return {
            **state,
            "processing_errors": processing_errors,
            "generation_complete": True,  # Stop on error
        }


def parallel_llm_generator_node(
    state: SyntheticAudienceState,
) -> SyntheticAudienceState:
    """Generate all behavioral content in parallel batches."""
    demographic_schedule = state["demographic_schedule"]
    processing_templates = state["processing_templates"]

    logger.info(
        f"Starting parallel generation for {len(demographic_schedule)} profiles"
    )

    try:
        # Initialize parallel batch processor
        batch_size = int(os.getenv("PARALLEL_BATCH_SIZE", "5"))
        provider = os.getenv("LLM_PROVIDER", "google")
        processor = ParallelBatchProcessor(batch_size=batch_size, provider=provider)

        # Process all profiles in parallel batches
        completed_profiles = processor.process_all_batches(
            demographic_schedule, processing_templates
        )

        logger.info(
            f"Parallel generation completed: {len(completed_profiles)} profiles generated"
        )

        return {
            **state,
            "completed_profiles": completed_profiles,
            "generation_complete": True,
        }

    except Exception as e:
        error_msg = f"Parallel generation failed: {str(e)}"
        logger.error(error_msg)

        processing_errors = state.get("processing_errors", [])
        processing_errors.append(error_msg)

        return {
            **state,
            "processing_errors": processing_errors,
            "generation_complete": True,
        }


def profile_assembler_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Assemble and validate final audience profiles."""
    logger.info("Assembling and validating final profiles...")

    completed_profiles = state["completed_profiles"]
    filter_details = state["filter_details"]

    # Validate profile count
    expected_count = filter_details.user_req_responses
    actual_count = len(completed_profiles)

    if actual_count != expected_count:
        error_msg = (
            f"Profile count mismatch: expected {expected_count}, got {actual_count}"
        )
        logger.error(error_msg)
        raise ValueError(error_msg)

    # Validate distribution accuracy
    distribution_accuracy = SyntheticAudienceGenerator._validate_distribution_static(
        completed_profiles, filter_details
    )

    generation_stats = {
        "total_profiles": len(completed_profiles),
        "generation_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "distribution_accuracy": distribution_accuracy,
    }

    logger.info(f"Successfully assembled {len(completed_profiles)} profiles")

    return {
        **state,
        "generation_stats": generation_stats,
        "assembly_complete": True,
    }


def output_writer_node(state: SyntheticAudienceState) -> SyntheticAudienceState:
    """Write final audience to JSON file."""
    logger.info("Writing output to JSON file...")

    completed_profiles = state["completed_profiles"]
    generation_stats = state["generation_stats"]
    output_file = state.get("output_file", "synthetic_audience.json")
    input_file = state.get("input_file", "unknown")

    # Prepare output data
    output_data = {
        "synthetic_audience": [profile.model_dump() for profile in completed_profiles],
        "generation_metadata": {
            **generation_stats,
            "input_file": input_file,
        },
    }

    # Write to file
    with open(output_file, "w") as f:
        json.dump(output_data, f, indent=2)

    logger.info(f"Output written to {output_file}")

    return {
        **state,
        "output_written": True,
        "output_path": output_file,
    }


# ============================================================================
# LANGGRAPH WORKFLOW SETUP
# ============================================================================


def should_continue_generation(state: SyntheticAudienceState) -> str:
    """Determine if generation should continue or move to assembly."""
    if state.get("generation_complete", False):
        return "profile_assembler"
    return "llm_generator"


def create_workflow() -> StateGraph:
    """Create and configure the LangGraph workflow."""
    logger.info("Creating LangGraph workflow...")

    # Create the graph
    workflow = StateGraph(SyntheticAudienceState)

    # Add nodes
    workflow.add_node("load_json", load_json_node)
    workflow.add_node("distribution_builder", distribution_builder_node)
    workflow.add_node("persona_processor", persona_processor_node)
    workflow.add_node("llm_generator", llm_generator_node)
    workflow.add_node("profile_assembler", profile_assembler_node)
    workflow.add_node("output_writer", output_writer_node)

    # Define edges
    workflow.set_entry_point("load_json")
    workflow.add_edge("load_json", "distribution_builder")
    workflow.add_edge("distribution_builder", "persona_processor")
    workflow.add_edge("persona_processor", "llm_generator")

    # Conditional edge for LLM iteration
    workflow.add_conditional_edges(
        "llm_generator",
        should_continue_generation,
        {
            "llm_generator": "llm_generator",  # Continue generating
            "profile_assembler": "profile_assembler",  # Move to assembly
        },
    )

    workflow.add_edge("profile_assembler", "output_writer")
    workflow.add_edge("output_writer", END)

    return workflow


def create_parallel_workflow() -> StateGraph:
    """Create and configure the parallel LangGraph workflow."""
    logger.info("Creating parallel LangGraph workflow...")

    # Create the graph
    workflow = StateGraph(SyntheticAudienceState)

    # Add nodes
    workflow.add_node("load_json", load_json_node)
    workflow.add_node("distribution_builder", distribution_builder_node)
    workflow.add_node("persona_processor", persona_processor_node)
    workflow.add_node("parallel_llm_generator", parallel_llm_generator_node)
    workflow.add_node("profile_assembler", profile_assembler_node)
    workflow.add_node("output_writer", output_writer_node)

    # Define edges - much simpler without the loop
    workflow.set_entry_point("load_json")
    workflow.add_edge("load_json", "distribution_builder")
    workflow.add_edge("distribution_builder", "persona_processor")
    workflow.add_edge("persona_processor", "parallel_llm_generator")
    workflow.add_edge("parallel_llm_generator", "profile_assembler")
    workflow.add_edge("profile_assembler", "output_writer")
    workflow.add_edge("output_writer", END)

    return workflow


# ============================================================================
# MAIN APPLICATION CLASS
# ============================================================================


class SyntheticAudienceGenerator:
    """Main application class orchestrating the generation process."""

    def __init__(self, use_parallel: bool = True):
        """Initialize the generator."""
        self.use_parallel = use_parallel

        if use_parallel:
            self.workflow = create_parallel_workflow()
            logger.info(
                "Synthetic Audience Generator initialized with PARALLEL LangGraph workflow"
            )
        else:
            self.workflow = create_workflow()
            logger.info(
                "Synthetic Audience Generator initialized with sequential LangGraph workflow"
            )

        # Compile the workflow
        self.app = self.workflow.compile()

    def process_request(self, input_file: str, output_file: str) -> Dict[str, Any]:
        """Process a complete generation request using LangGraph workflow."""
        logger.info(f"Processing request from {input_file} using LangGraph workflow")

        # Initial state
        initial_state = {
            "input_file": input_file,
            "output_file": output_file,
        }

        # Run the workflow
        try:
            final_state = None
            # Set high recursion limit for large batch processing
            config = {"recursion_limit": 1000}
            for step in self.app.stream(initial_state, config=config):
                node_name = list(step.keys())[0]
                final_state = step[node_name]

                # Progress tracking for LLM generation
                if (
                    node_name == "llm_generator"
                    and "current_profile_index" in final_state
                ):
                    current = final_state["current_profile_index"]
                    total = len(final_state.get("demographic_schedule", []))
                    if total > 0:
                        progress = (current / total) * 100
                        print(
                            f"Generation progress: {progress:.1f}% ({current}/{total})"
                        )

                logger.info(f"Completed node: {node_name}")

            # Check for errors
            if final_state and final_state.get("processing_errors"):
                raise Exception(
                    f"Processing errors: {final_state['processing_errors']}"
                )

            # Return generation metadata
            return final_state.get("generation_stats", {})

        except Exception as e:
            logger.error(f"Workflow execution failed: {str(e)}")
            raise

    @staticmethod
    def _validate_distribution_static(
        profiles: List[SyntheticProfile], filter_details: FilterDetails
    ) -> Dict[str, Any]:
        """Static method to validate distribution (for use in nodes)."""
        total_profiles = len(profiles)

        # Count actual distributions
        gender_counts = {}
        age_counts = {}
        ethnicity_counts = {}

        for profile in profiles:
            # Gender
            gender_counts[profile.gender] = gender_counts.get(profile.gender, 0) + 1
            # Age
            age_counts[profile.age_bucket] = age_counts.get(profile.age_bucket, 0) + 1
            # Ethnicity
            ethnicity_counts[profile.ethnicity] = (
                ethnicity_counts.get(profile.ethnicity, 0) + 1
            )

        # Calculate actual percentages
        gender_percentages = {
            k: round((v / total_profiles) * 100, 1) for k, v in gender_counts.items()
        }
        age_percentages = {
            k: round((v / total_profiles) * 100, 1) for k, v in age_counts.items()
        }
        ethnicity_percentages = {
            k: round((v / total_profiles) * 100, 1) for k, v in ethnicity_counts.items()
        }

        return {
            "total_profiles": total_profiles,
            "expected_total": filter_details.user_req_responses,
            "gender_distribution": {
                "expected": filter_details.gender_proportions,
                "actual_counts": gender_counts,
                "actual_percentages": gender_percentages,
            },
            "age_distribution": {
                "expected": filter_details.age_proportions,
                "actual_counts": age_counts,
                "actual_percentages": age_percentages,
            },
            "ethnicity_distribution": {
                "expected": filter_details.ethnicity_proportions,
                "actual_counts": ethnicity_counts,
                "actual_percentages": ethnicity_percentages,
            },
        }

    def visualize_workflow(self, save_path: str = None, display_image: bool = True):
        """
        Visualize the LangGraph workflow using Mermaid diagram.

        Args:
            save_path: Optional path to save the PNG image
            display_image: Whether to display the image (requires IPython)

        Returns:
            Image object if successful, None otherwise
        """
        try:
            logger.info("Generating workflow visualization...")

            # Generate Mermaid PNG
            mermaid_png = self.app.get_graph().draw_mermaid_png()

            if save_path:
                # Save to file
                with open(save_path, "wb") as f:
                    f.write(mermaid_png)
                logger.info(f"Workflow diagram saved to {save_path}")

            if display_image and IPYTHON_AVAILABLE:
                # Display in Jupyter/IPython environment
                img = Image(mermaid_png)
                display(img)
                return img
            elif display_image:
                logger.warning("IPython not available - cannot display image inline")
                logger.info("Consider saving to file using save_path parameter")

            return mermaid_png

        except Exception as e:
            logger.error(f"Failed to generate workflow visualization: {str(e)}")
            return None

    def get_workflow_mermaid_code(self) -> str:
        """
        Get the Mermaid code representation of the workflow.

        Returns:
            Mermaid diagram code as string
        """
        try:
            return self.app.get_graph().draw_mermaid()
        except Exception as e:
            logger.error(f"Failed to generate Mermaid code: {str(e)}")
            return ""


# ============================================================================
# CLI INTERFACE
# ============================================================================


@click.command()
@click.option("--input", "-i", help="Input JSON file path")
@click.option("--output", "-o", help="Output JSON file path")
@click.option(
    "--validate-env", is_flag=True, help="Run environment validation before processing"
)
@click.option(
    "--visualize", is_flag=True, help="Generate and display workflow visualization"
)
@click.option("--save-graph", help="Save workflow graph to specified PNG file path")
@click.option(
    "--show-mermaid", is_flag=True, help="Print Mermaid diagram code to console"
)
@click.option(
    "--parallel/--sequential",
    default=True,
    help="Use parallel processing (default) or sequential processing",
)
@click.option(
    "--batch-size",
    type=int,
    default=DEFAULT_BATCH_SIZE,
    help=f"Batch size for parallel processing (default: {DEFAULT_BATCH_SIZE})",
)
@click.option(
    "--max-workers",
    type=int,
    default=DEFAULT_MAX_WORKERS,
    help=f"Maximum worker threads (default: {DEFAULT_MAX_WORKERS})",
)
@click.option(
    "--provider",
    type=click.Choice(["google", "azure"]),
    default=DEFAULT_LLM_PROVIDER,
    help=f"LLM provider to use (default: {DEFAULT_LLM_PROVIDER})",
)
def main(
    input: str,
    output: str,
    validate_env: bool,
    visualize: bool,
    save_graph: str,
    show_mermaid: bool,
    parallel: bool,
    batch_size: int,
    max_workers: int,
    provider: str,
):
    """Synthetic Audience Generator MVP - Generate synthetic audience profiles."""

    if validate_env:
        logger.info("Running environment validation...")
        os.system("python validate_environment.py")

    try:
        # Set environment variables for processing configuration
        os.environ["LLM_PROVIDER"] = provider

        if parallel:
            os.environ["PARALLEL_BATCH_SIZE"] = str(batch_size)
            os.environ["MAX_WORKERS"] = str(max_workers)
            logger.info(
                f"Parallel processing enabled: provider={provider}, batch_size={batch_size}, max_workers={max_workers}"
            )
        else:
            logger.info(f"Sequential processing enabled: provider={provider}")

        generator = SyntheticAudienceGenerator(use_parallel=parallel)

        # Handle visualization options
        if visualize or save_graph or show_mermaid:
            logger.info("Processing visualization requests...")

            if show_mermaid:
                print("\n" + "=" * 60)
                print("LANGGRAPH WORKFLOW MERMAID CODE")
                print("=" * 60)
                mermaid_code = generator.get_workflow_mermaid_code()
                if mermaid_code:
                    print(mermaid_code)
                else:
                    print("Failed to generate Mermaid code")
                print("=" * 60)

            if visualize or save_graph:
                save_path = save_graph if save_graph else None
                result = generator.visualize_workflow(
                    save_path=save_path, display_image=visualize
                )
                if result is None:
                    print("âŒ Failed to generate workflow visualization")
                elif save_path:
                    print(f"âœ… Workflow diagram saved to: {save_path}")

            # If only visualization was requested, exit here
            if not input or not output:
                print("\nâœ… Visualization completed")
                return

        # Validate required arguments for processing
        if not input or not output:
            print("âŒ Error: --input and --output are required for processing")
            print("Use --help for more information")
            exit(1)

        metadata = generator.process_request(input, output)

        print("\n" + "=" * 60)
        print("âœ… GENERATION COMPLETED SUCCESSFULLY")
        print("=" * 60)
        print(f"Total Profiles Generated: {metadata['total_profiles']}")
        print(f"Expected Total: {metadata['distribution_accuracy']['expected_total']}")
        print(f"Output File: {output}")
        print(f"Generation Time: {metadata['generation_timestamp']}")

        # Print distribution summary
        print("\nDistribution Accuracy:")
        print(
            f"Gender: {metadata['distribution_accuracy']['gender_distribution']['actual_percentages']}"
        )
        print(
            f"Age: {metadata['distribution_accuracy']['age_distribution']['actual_percentages']}"
        )
        print(
            f"Ethnicity: {metadata['distribution_accuracy']['ethnicity_distribution']['actual_percentages']}"
        )

    except Exception as e:
        logger.error(f"Generation failed: {str(e)}")
        print(f"\nâŒ GENERATION FAILED: {str(e)}")
        exit(1)


if __name__ == "__main__":
    main()
